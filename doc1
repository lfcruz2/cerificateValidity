
---

## **Project Plan: End-to-End ML Pipeline**

| **Phase & Step**                       | **Task Description**                                                                                                      | **Key Activities**                                                                                                                                                                                                                                                                        | **Deliverables**                                                                                                                       |
|:---------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------|
| **Phase 1: Data Preparation**          | **1.1 Collect Raw & C&A Transactions**                                                                                    | - Gather raw transactions (logs, CSVs, DB exports)<br>- Collect C&A transactions from subject matter experts<br>- Consolidate data in a single location                                                                                                                                    | - Initial raw dataset compilation<br>- Confirmed data sources and file structures                                                                                            |
|                                       | **1.2 Validate Data (IDs & Timestamps)**                                                                                  | - Match canceled & replacement transactions by ID/time<br>- Remove invalid or incomplete pairs<br>- Document assumptions & data issues                                                                                                                                                    | - Validated dataset with consistent ID/time pairs<br>- Log of discarded/improper data                                                                                        |
|                                       | **1.3 Create Master DataFrame**                                                                                           | - Merge canceled & replaced rows into paired records<br>- Assign unique IDs to each pair<br>- Ensure columns are appropriately labeled                                                                                                                                                | - “Master” DataFrame (or table) containing all relevant fields for each pair<br>- Data dictionary capturing newly formed columns                                           |
|                                       | **1.4 Clean & Preprocess**                                                                                                | - Handle missing values, outliers, duplicates<br>- Convert data types (numeric, text, datetime)<br>- Split textual fields if needed (e.g., tokens, categories)                                                                                                                             | - Cleaned, standardized dataset ready for analysis<br>- Data quality report (missing values, outlier handling, etc.)                                                        |
| **Phase 2: Exploratory Data Analysis** | **2.1 Numeric Analysis**                                                                                                   | - Generate summary statistics (mean, median, std, etc.)<br>- Plot histograms, boxplots, correlation matrices<br>- Identify potential outliers and data distribution patterns                                                                                                               | - Numerical EDA summary (figures, correlation heatmaps)<br>- Notes on potential issues or anomalies                                                                          |
|                                       | **2.2 Text & Time Analysis**                                                                                              | - Investigate text fields (common words, token distributions)<br>- Identify daily/weekly/monthly patterns in timestamps<br>- Perform preliminary grouping or categorization as needed                                                                                                    | - Text distribution insights (frequent terms, potential categories)<br>- Time-based analysis (peak hours, daily/weekly trends)                                              |
|                                       | **2.3 Feature Engineering**                                                                                               | - Devise similarity measures (numeric distances, text embeddings, etc.)<br>- Create new time-based features (e.g., time deltas, day-of-week)<br>- Encode categorical data as needed (one-hot, label encoding)                                                                             | - Expanded dataset with new feature columns<br>- Documentation of how each feature was created                                                                               |
| **Phase 3: Modeling**                 | **3.1 Initial Training**                                                                                                  | - Train basic models (e.g., logistic regression, random forest) on numeric/text/time features separately<br>- Establish baseline metrics (accuracy, precision, recall, F1, etc.)<br>- Quickly evaluate model fit                                                                         | - Initial performance metrics (baseline results)<br>- Model artifacts (checkpoints) for reference                                                                            |
|                                       | **3.2 Record Results**                                                                                                    | - Store metrics in a systematic format (Excel, CSV, or DB table)<br>- Compare quick wins across different model types                                                                                                                              | - Metrics log (accuracy, recall, etc.)<br>- Possibly a “best so far” candidate model                                                                                         |
|                                       | **3.3 Test Various Columns**                                                                                              | - Use a config file listing different feature sets (numeric-only, text-only, combined, etc.)<br>- Automate training & evaluation in a loop<br>- Store each run’s performance metrics                                                               | - Detailed comparison table of feature-set vs. performance<br>- Automated script for iterative training                                                                     |
|                                       | **3.4 Hyperparameter Tuning**                                                                                             | - Focus on top-performing feature combos from previous step<br>- Use grid search or random search to fine-tune hyperparameters (e.g., learning rate, max depth)<br>- Cross-validate results for robustness                                                                               | - Optimized models with documented hyperparameter settings<br>- Updated performance metrics                                                                                 |
|                                       | **3.5 Select Best Model**                                                                                                 | - Identify which model + feature set + hyperparams yield highest performance<br>- Save final model object (serialized format) for deployment<br>- Produce summary of final metrics                                                                | - “Best model” artifact (e.g., .pkl or .h5 file)<br>- Final metric report with chosen hyperparameters                                                                        |
| **Phase 4: Deployment**               | **4.1 Build FastAPI App**                                                                                                 | - Implement endpoints for training and evaluation (optionally loading config/hyperparams)<br>- Integrate Swagger (OpenAPI) docs for interactive testing<br>- Set up basic logging and error handling                                                                                     | - FastAPI project structure<br>- /train, /evaluate endpoints with automated docs                                                                                             |
|                                       | **4.2 Inference Endpoint**                                                                                               | - Create an endpoint to input canceled transaction data<br>- Return top 5 potential replacements based on the deployed model<br>- Log inference requests for tracking and debugging                                                                                                       | - /predict or /inference endpoint returning predictions<br>- Logging/analytics for real-time usage                                                                           |
|                                       | **4.3 Refine & Iterate**                                                                                                  | - Gather user feedback on inference quality<br>- Retrain model or adjust hyperparameters as new data arrives<br>- Continuously track performance and usability                                                                                       | - Iteration plan with improved model versions<br>- Long-term roadmap for incremental updates                                                                                |

---
